{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MDPs in pymdptoolbox - Class Assignment\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this practical exercise, we will look at how we can implement MDP planning in a mathematical toolkit, and track the calculation of the rewards for each state via Value Iteration. The following code sets up an MDP environment (the basic case shown in class, shown in the Figure below) and computes the policy for the given MDP using the Value Iteration algorithm.\n",
    "\n",
    "<img align=\"center\" src=\"mdp_simple.png\"/>\n",
    "\n",
    "While you can find the code to solve all the questions from this notebook within this repository (if you inspect the files), we strongly suggest you try to solve the coding problems yourself first.\n",
    "\n",
    "Then we provide a set of questions for you to implement and answer. This assignment is **not graded**.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# The line below is to be used if you have pymdptoolbox installed with setuptools\n",
    "# import mdptoolbox.example\n",
    "# Whereas the line below obviate the need to install that\n",
    "import sys\n",
    "sys.path.insert(1,'pymdptoolbox/src')\n",
    "import mdptoolbox.example\n",
    "\n",
    "import numpy as _np\n",
    "from gen_scenario import *\n",
    "\n",
    "\"\"\"\n",
    "(Y,X)\n",
    "| 00 01 02 ... 0X-1       'N' = North\n",
    "| 10  .         .         'S' = South\n",
    "| 20    .       .         'W' = West\n",
    "| .       .     .         'E' = East\n",
    "| .         .   .         'T' = Terminal\n",
    "| .           . .         'O' = Obstacle\n",
    "| Y-1,0 . . .   Y-1X-1\n",
    "\"\"\" \n",
    "\n",
    "shape = [3,4]\n",
    "rewards = [[0,3,100],[1,3,-100]]\n",
    "obstacles = [[1,1]]\n",
    "terminals = [[0,3],[1,3]]\n",
    "P, RSS, R = mdp_grid(shape=shape, terminals=terminals, r=-3, rewards=rewards, obstacles=obstacles)\n",
    "vi = mdptoolbox.mdp.ValueIterationGS(P, R, discount=0.99, epsilon=0.001, max_iter=1000, skip_check=True)\n",
    "\n",
    "vi.run()\n",
    "#You can check the quadrant values using print vi.V\n",
    "# print_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)\n",
    "display_policy(vi.policy, shape, obstacles=obstacles, terminals=terminals)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Converting Rewards\n",
    "\n",
    "In the lecture, we saw that the reward function can be of one of two forms:\n",
    "\n",
    "- $R(s): \\mathcal{S} \\mapsto \\mathbb{R}$ - where rewards are associated to a single state\n",
    "- $R(s,a): \\mathcal{S} \\times \\mathcal{A} \\mapsto \\mathbb{R}$ - where rewards accrue for pairs of states and actions\n",
    "\n",
    "Your next piece of code will convert from the state-based reward into a state-action immediate reward. In this case, you can consider that the reward of a state-action pair $(s,a)$  for a state $s$ and action $a$ is the immediate reward of the neighboring states $s'$ weighed by the probability of reaching them using action $a$, thus:\n",
    "\n",
    "$$R(s,a) = \\sum_{s'}R(s')P(s,a,s')$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Don't forget we import numpy as _np\n",
    "def r_to_ra(P, R):\n",
    "    # We start by assuming we have a fixed number of actions (4, the cardinal points)\n",
    "    # So we need a matrix with SxA dimensions to store the rewards\n",
    "    RS = _np.zeros([len(P[1]), 4])\n",
    "    # You will now convert the rewards for single states into rewards for state-action pairs\n",
    "    ## YOUR CODE START\n",
    "    \n",
    "    ## YOUR CODE END\n",
    "    return RS"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next cell tests the code you just developed to ensure your conversion is sound."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shape = [2,2]\n",
    "rewards = [[1,1,1]]\n",
    "obstacles = []\n",
    "terminals = [[1,1]]\n",
    "Ptest, RSStest, Rtest = mdp_grid(shape=shape, terminals=terminals, r=-1, rewards=rewards, obstacles=obstacles)\n",
    "\n",
    "# Let's compare your implementation with the reference one\n",
    "RA = r_to_ra(Ptest,Rtest)\n",
    "RA2 = r_to_rs(Ptest,Rtest,terminals,obstacles,shape)\n",
    "print(\"Expected:\", str(RA2))\n",
    "print(\"Actual:\", str(RA))\n",
    "assert((RA == RA2).all())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Bellman Operator\n",
    "\n",
    "We are now going to implement the Bellman Operator, recall that there are many forms of the Bellman equation. At its simplest, it specifies, for each state, the expected discounted reward of a policy as a function of the value of their reachable neighbors.\n",
    "$$v_{\\pi}(s) = \\mathcal{R}_{s}^{\\pi(s)} + \\gamma\\sum_{s' \\in \\mathcal{S}}\\mathcal{P}_{ss'}^{\\pi(a)}v_{\\pi}(s)$$\n",
    "\n",
    "Similarly, the Bellman optimality equation consists of choosing actions greedily at each state:\n",
    "$$v_{*}(s) = \\max_{a \\in \\mathcal{A}}\\left[\\mathcal{R}_{s}^{a} + \\gamma\\sum_{s' \\in \\mathcal{S}}\\mathcal{P}_{ss'}^{a}v_{*}(s)\\right]$$\n",
    "\n",
    "Below, you will implement the Bellman Optimality operator assuming a reward function of the form $R(s,a)$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Bellman operator\n",
    "\n",
    "\n",
    "def bellmanOperator(P, R, V, discount=0.9):\n",
    "    \"\"\"The Bellman Optimality Operator. \n",
    "    input:\n",
    "    P -> AxSxS transition matrix indicating P(s' | s, a)\n",
    "    R -> AxS reward function indicading R(S,A)\n",
    "    V -> S a value function in matrix form\n",
    "    discount -> [0,1] the discount factor\n",
    "    \"\"\"\n",
    "    Vp = _np.zeros(P.shape[0])\n",
    "    ## YOUR CODE START\n",
    "\n",
    "    ## YOUR CODE END\n",
    "    return pi, Vp\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will now test your implementation of the Bellman operator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from mdptoolbox.mdp import MDP\n",
    "\n",
    "shape = [2,2]\n",
    "rewards = [[1,1,1]]\n",
    "obstacles = []\n",
    "terminals = [[1,1]]\n",
    "Ptest, RSStest, Rtest = mdp_grid(shape=shape, terminals=terminals, r=-1, rewards=rewards, obstacles=obstacles)\n",
    "\n",
    "# Let's compare your implementation with the reference one\n",
    "V = _np.zeros(4)\n",
    "pi1, V1 = bellmanOperator(Ptest, RSStest, V, 0.9)\n",
    "mdp = MDP(Ptest,RSStest,.9,.001,10,False)\n",
    "pi2, V2 = mdp._bellmanOperator(V)\n",
    "\n",
    "print(\"Expected V:\", str(V2))\n",
    "print(\"Actual V:\", str(V1))\n",
    "assert((V1 == V2).all())\n",
    "print(\"Expected policy\")\n",
    "display_policy(pi2, shape, obstacles=obstacles, terminals=terminals)\n",
    "print(\"Actual policy\")\n",
    "display_policy(pi1, shape, obstacles=obstacles, terminals=terminals)\n",
    "assert((pi1 == pi2).all())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Value Iteration\n",
    "\n",
    "Finally, we will implement a simplified version of the value iteration algorithm, which consists of using the Bellman optimality equation (which you just implemented) until the value function converges to within an error bound $\\epsilon$.\n",
    "At each iteration, you should compare the ```max_norm``` difference between consecutive value functions (which we call ```delta```, or $\\delta$), with the bound established by the algorithm $\\frac{\\epsilon(1 - \\gamma)}{\\gamma}$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# This returns the span of an array\n",
    "def max_norm(a):\n",
    "    return a.max() - a.min()\n",
    "\n",
    "def valueIteration(P, R, discount=0.9, epsilon=0.01):\n",
    "    delta = float(\"inf\")\n",
    "    v = _np.zeros(R.shape[1])\n",
    "    pi = _np.zeros(R.shape[1])\n",
    "    ## YOUR CODE START\n",
    "\n",
    "    ## YOUR CODE END\n",
    "    return pi, v\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now test your implementation of the value iteration algorithm against the reference implementation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from mdptoolbox.mdp import MDP\n",
    "\n",
    "shape = [2,2]\n",
    "rewards = [[1,1,1]]\n",
    "obstacles = []\n",
    "terminals = [[1,1]]\n",
    "Ptest, RSStest, Rtest = mdp_grid(shape=shape, terminals=terminals, r=-1, rewards=rewards, obstacles=obstacles)\n",
    "\n",
    "pi1, v1 = valueIteration(Ptest,RSStest, 0.9, 0.01)\n",
    "vi = mdptoolbox.mdp.ValueIteration(Ptest, RSStest, discount=0.9, epsilon=0.01, max_iter=1000, skip_check=True)\n",
    "vi.run()\n",
    "\n",
    "pi2 = vi.policy\n",
    "v2 = vi.V\n",
    "print(\"Expected V:\", str(v2))\n",
    "print(\"Actual V:\", str(v1))\n",
    "assert((v1 == v2).all())\n",
    "\n",
    "print(\"Expected policy\")\n",
    "display_policy(pi2, shape, obstacles=obstacles, terminals=terminals)\n",
    "print(\"Actual policy\")\n",
    "display_policy(pi1, shape, obstacles=obstacles, terminals=terminals)\n",
    "assert((pi1 == pi2).all())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we plot the difference between each iteration of the Value Iteration algorithm using many different discount factors. To plot this image, you must install seaborn. To install seaborn run: \n",
    "\n",
    "```\n",
    "pip3 install seaborn --user\n",
    "\n",
    "```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot(P, R, discounts=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95], epsilon=0.001, max_iter=1000):\n",
    "    data_list = []\n",
    "    import pandas as pd\n",
    "    from matplotlib import pyplot as plt\n",
    "    %matplotlib inline\n",
    "    import seaborn as sns\n",
    "    data_list = []\n",
    "    for d in discounts:\n",
    "        vis = mdptoolbox.mdp.ValueIteration(P, R, d, epsilon, max_iter, skip_check=True)\n",
    "        vis.run()\n",
    "        iterations = 1\n",
    "        for value in vis.iterations_list:\n",
    "            data_list.append([value, 'gamma: ' + str(d), iterations])\n",
    "            iterations +=1\n",
    "    data_frame2 = pd.DataFrame(data_list, columns=['difference', 'discount', 'iterations'])\n",
    "    ax = sns.relplot(x = 'iterations', y = 'difference', hue='discount', kind=\"line\", data=data_frame2)\n",
    "plot(P,R)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use the following commands to check the difference of value-function values during each teration and the values of each value function.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(vi.iterations_list)\n",
    "print(vi.v_list[-1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before you go for the questionnaire, take your time to open the source code of the MDP toolkit we use, specifically, look into these files:\n",
    "1. [gen_scenario.py](gen_scenario.py) - contains the conversion code to make the simple coordinate commands above (e.g. ```shape = [3,4]```) into the matrices actually used by the MDP solver\n",
    "2. [mdp.py](pymdptoolbox/src/mdptoolbox/mdp.py) - contains most of the logic for an MDP, including the *Bellman Equation* as follows:\n",
    "\n",
    "$$V(s) = \\left[ \\max_{a} \\gamma \\sum_{s'}P(s'|s,a)*V(s') \\right]+ R(s)$$\n",
    "\n",
    "See if you can identify how this equation is implemented in the ```MDP._bellmanOperator``` with the [mdp.py](pymdptoolbox/src/mdptoolbox/mdp.py) file. Note how this implementation uses matrix multiplication to achieve the summation step described in the equation. Once you believe you understand that, go ahead and respond the questionnaire. \n",
    "\n",
    "### Questionnaire\n",
    "1. Study the code of the cell above and answer the following questions.\n",
    "\t1. What is the policy generated if we change the discount factor of the grid domain to ```0.1```?\n",
    "\t2. Use the following line ```vi.verbose = True``` before ```vi.run()```:   \n",
    "\tWhat is the variation for each of the first three iterations with the discount factor of ```0.9``` and how many iterations does the algorithm take to converge?\n",
    "\t3. How does changes to the discount factor affect the variation of the state values over time?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#1.A"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#1.B"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#1.C"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. The scenario below has an interesting structure whereby the positive rewarding terminal state is partially surrounded by negatively-rewarding states. Program this scenario in pymdptoolbox and compute the optimal policy with a discount factor of 0.99.\n",
    "\n",
    "<img align=\"center\" src=\"mdp-odd.png\"/>\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Define two new 5 by 5 scenarios with multiple obstacles and an interesting geometry following the guidelines below. Calculate the policy with discount factor 0.99, and then try to explain intuitively the reason for the resulting policies, given the initial parameters. These two scenarios must have the following characteristics:\n",
    "\t1. A scenario with one (or more) terminal states with positive rewards and at least one other state with the same amount of, but negative reward and no terminal states with negative rewards.\n",
    "\t2. A scenario with one terminal state with a negative reward and at least one non-terminal state with a positive reward."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#3.A"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#3.B"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "interpreter": {
   "hash": "9710f9ce428c345a588eaab997638b0f061789d22b5dda6b7c1337a7732af91d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}